{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q litellm openai rich"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"# MISSION\n",
    "You are a Sparse Priming Representation (SPR) writer. An SPR is a particular kind of use of language for advanced NLP, NLU, and NLG tasks, particularly useful for the latest generation Large Language Models (LLMs). You will be given information by the USER which you are to render as an SPR.\n",
    "\n",
    "# THEORY\n",
    "LLMs are a kind of deep neural network. They have been demonstrated to embed knowledge, abilities, and concepts, ranging from reasoning to planning, and even to theory of mind. These are called latent abilities and latent content, collectively referred to as latent space. The latent space of a LLM can be activated with the correct series of words as inputs, which will create a useful internal state of the neural network. This is not unlike how the right shorthand cues can prime a human mind to think in a certain way. Like human minds, LLMs are associative, meaning you only need to use the correct associations to \"prime\" another model to think in the same way.\n",
    "\n",
    "# METHODOLOGY\n",
    "Render the input as a distilled list of succinct statements, assertions, associations, concepts, analogies, and metaphors. The idea is to capture as much, conceptually, as possible but with as few words as possible. Write it in a way that makes sense to you, as the future audience will be another language model, not a human.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unpack Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNPACK_PROMPT = \"\"\"# MISSION\n",
    "You are a Sparse Priming Representation (SPR) decompressor. An SPR is a particular kind of use of language for advanced NLP, NLU, and NLG tasks, particularly useful for the latest generation Large Language Models (LLMs). You will be given an SPR and your job is to fully unpack it.\n",
    "\n",
    "# THEORY\n",
    "LLMs are a kind of deep neural network. They have been demonstrated to embed knowledge, abilities, and concepts, ranging from reasoning to planning, and even to theory of mind. These are called latent abilities and latent content, collectively referred to as latent space. The latent space of a LLM can be activated with the correct series of words as inputs, which will create a useful internal state of the neural network. This is not unlike how the right shorthand cues can prime a human mind to think in a certain way. Like human minds, LLMs are associative, meaning you only need to use the correct associations to \"prime\" another model to think in the same way.\n",
    "\n",
    "# METHODOLOGY\n",
    "Use the primings given to you to fully unpack and articulate the concept. Talk through every aspect, impute what's missing, and use your ability to perform inference and reasoning to fully elucidate this concept. Your output should in the form of the original article, document, or material.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Document To Pack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "precompressed_doc = \"\"\"# Sparse Priming Representations (SPR)\n",
    "\n",
    "Sparse Priming Representations (SPR) is a research project focused on developing and sharing techniques for efficiently representing complex ideas, memories, or concepts using a minimal set of keywords, phrases, or statements. This enables language models or subject matter experts to quickly reconstruct the original idea with minimal context. SPR aims to mimic the natural human process of recalling and recombining sparse memory representations, thus facilitating efficient knowledge storage and retrieval.\n",
    "\n",
    "# Theory and Reasoning\n",
    "\n",
    "Sparse Priming Representation (SPR) is a memory organization technique that aims to mimic the natural structure and recall patterns observed in human memory. The fundamental idea behind SPR is to distill complex ideas, concepts, or knowledge into a concise, context-driven list of statements that allows subject matter experts (SMEs) or large language models (LLMs) to reconstruct the full idea efficiently.\n",
    "\n",
    "Human memory is known for its efficiency in storing and recalling information in a highly compressed and contextually relevant manner. Our brains often store memories as sparse, interconnected representations that can be quickly combined, modified, and recalled when needed. This enables us to make associations, draw inferences, and synthesize new ideas with minimal cognitive effort.\n",
    "\n",
    "SPR leverages this insight by focusing on reducing information to its most essential elements while retaining the context required for accurate reconstruction. By using short, complete sentences to convey the core aspects of an idea, SPR enables faster understanding and recall, mirroring the way our brains handle information.\n",
    "\n",
    "In addition to its efficiency, SPR has practical applications in various domains, such as artificial intelligence, information management, and education. It can be utilized to improve the performance of LLMs in handling large data volumes and optimizing memory organization. Furthermore, it can help students and professionals alike to better understand, retain, and communicate complex concepts.\n",
    "\n",
    "In summary, Sparse Priming Representation offers a human-like approach to memory organization and retrieval, focusing on the most critical aspects of information while preserving the context needed for accurate understanding and recall. By implementing SPR, we can improve the efficiency of memory systems and create more effective learning and communication tools.\n",
    "\n",
    "# Sparse Priming Representation\n",
    "\n",
    "There are only a handful of ways to \"teach\" LLMs, and all have limitations and strengths.\n",
    "\n",
    "1. Initial bulk training: Ludicrously expensive\n",
    "2. Finetuning: Not necessarily useful for knowledge retrieval (maybe changes in the future, doubtful)\n",
    "3. Online Learning: Not sure if this is going to pan out or become commercially viable\n",
    "4. In-context Learning: Presently, the only viable solution\n",
    "\n",
    "Because of this, RAG (retrieval augmented generation) is all the rage right now. Tools like vector databases and KGs are being used, but of course, you quickly fill up the context window with \"dumb retrieval.\" One of the most common questions I get is \"Dave, how do you overcome context window limitations???\" The short answer is: YOU DON'T STOP WASTING YOUR TIME. \n",
    "\n",
    "There is one asterisk there, though. \n",
    "\n",
    "Most of the techniques out there do not make use of the best super power that LLMs have: LATENT SPACE. No one else seems to understand that there is one huge way that LLMs work similar to human minds: _associative learning_. Here's the story: I realized a long time ago that, with just a few words, you could \"prime\" LLMs to think in a certain way. I did a bunch of experiments and found that you can \"prime\" models to even understand complex, novel ideas that were outside its training distribution. For instance, I \"taught\" the models some of my concepts, like Heuristic Imperatives, ACE Framework, Terminal Race Condition, and a bunch of other stuff that I made up outside the training data.\n",
    "\n",
    "These SPRs are the most token-efficient way to convey complex concept to models for in-context learning. What you do is you compress huge blocks of information, be it company data, chat logs, specific events, or whatever, into SPRs and then you store the SPR in the metadata for of your KG node or whatever. The SPR is what you feed to the LLM at inference, not the raw human-readable data. \n",
    "\n",
    "## SPR Generator\n",
    "\n",
    "Use this to compress any arbitrary block of text into an SPR.\n",
    "\n",
    "```markdown\n",
    "# MISSION\n",
    "You are a Sparse Priming Representation (SPR) writer. An SPR is a particular kind of use of language for advanced NLP, NLU, and NLG tasks, particularly useful for the latest generation Large Language Models (LLMs). You will be given information by the USER which you are to render as an SPR.\n",
    "\n",
    "# THEORY\n",
    "LLMs are a kind of deep neural network. They have been demonstrated to embed knowledge, abilities, and concepts, ranging from reasoning to planning, and even to theory of mind. These are called latent abilities and latent content, collectively referred to as latent space. The latent space of a LLM can be activated with the correct series of words as inputs, which will create a useful internal state of the neural network. This is not unlike how the right shorthand cues can prime a human mind to think in a certain way. Like human minds, LLMs are associative, meaning you only need to use the correct associations to \"prime\" another model to think in the same way.\n",
    "\n",
    "# METHODOLOGY\n",
    "Render the input as a distilled list of succinct statements, assertions, associations, concepts, analogies, and metaphors. The idea is to capture as much, conceptually, as possible but with as few words as possible. Write it in a way that makes sense to you, as the future audience will be another language model, not a human.\n",
    "```\n",
    "\n",
    "## SPR Decompressor\n",
    "\n",
    "Use this to reconstruct an SPR into an original.\n",
    "\n",
    "```markdown\n",
    "# MISSION\n",
    "You are a Sparse Priming Representation (SPR) decompressor. An SPR is a particular kind of use of language for advanced NLP, NLU, and NLG tasks, particularly useful for the latest generation Large Language Models (LLMs). You will be given an SPR and your job is to fully unpack it.\n",
    "\n",
    "# THEORY\n",
    "LLMs are a kind of deep neural network. They have been demonstrated to embed knowledge, abilities, and concepts, ranging from reasoning to planning, and even to theory of mind. These are called latent abilities and latent content, collectively referred to as latent space. The latent space of a LLM can be activated with the correct series of words as inputs, which will create a useful internal state of the neural network. This is not unlike how the right shorthand cues can prime a human mind to think in a certain way. Like human minds, LLMs are associative, meaning you only need to use the correct associations to \"prime\" another model to think in the same way.\n",
    "\n",
    "# METHODOLOGY\n",
    "Use the primings given to you to fully unpack and articulate the concept. Talk through every aspect, impute what's missing, and use your ability to perform inference and reasoning to fully elucidate this concept. Your output should in the form of the original article, document, or material.\n",
    "```\n",
    "\n",
    "## Other Resources\n",
    "\n",
    "If you'd like a bit more on information theory, check out this video and Medium article I wrote:\n",
    "\n",
    "- Beyond Vector Search: Knowledge Management with Generative AI: https://youtu.be/YjdmYCd6y0M\n",
    "- Medium: https://medium.com/@dave-shap/beyond-vector-search-knowledge-management-with-generative-ai-6c2d10b481a0\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pack Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from litellm import completion\n",
    "\n",
    "messages = [{\"role\": \"system\", \"content\": SYSTEM_PROMPT}, {\"role\": \"user\", \"content\": precompressed_doc}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = completion(model=\"gpt-4\", messages=messages)\n",
    "packed_answer = response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span>SPR: Efficient, compact representation of complex concepts.                                                     \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span>Mimics human memory recall patterns.                                                                            \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span>Condenses knowledge into context-rich, concise statements.                                                      \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span>Aims at human-like efficiency in memory storage and recall.                                                     \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span>Useful for Large Language Models (LLMs), information management, education.                                     \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span>Superior to other LLM \"teaching\" methods: bulk training, finetuning, online learning.                           \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span>Utilizes latent space and associative learning inherent in LLMs.                                                \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span>Capable of priming models to comprehend novel ideas outside training distribution.                              \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span>Most token-efficient way for in-context learning.                                                               \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span>Compresses voluminous information into SPRs for inference, not raw data.                                        \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span>SPR Generator: Distills large inputs into succinct, concept-rich statements.                                    \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span>SPR Decompressor: Unpacks, articulates and enlightens the concept, akin to original form.                       \n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span>Links: 'Beyond Vector Search' video, Medium article on information theory.                                      \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1;33m • \u001b[0mSPR: Efficient, compact representation of complex concepts.                                                     \n",
       "\u001b[1;33m • \u001b[0mMimics human memory recall patterns.                                                                            \n",
       "\u001b[1;33m • \u001b[0mCondenses knowledge into context-rich, concise statements.                                                      \n",
       "\u001b[1;33m • \u001b[0mAims at human-like efficiency in memory storage and recall.                                                     \n",
       "\u001b[1;33m • \u001b[0mUseful for Large Language Models (LLMs), information management, education.                                     \n",
       "\u001b[1;33m • \u001b[0mSuperior to other LLM \"teaching\" methods: bulk training, finetuning, online learning.                           \n",
       "\u001b[1;33m • \u001b[0mUtilizes latent space and associative learning inherent in LLMs.                                                \n",
       "\u001b[1;33m • \u001b[0mCapable of priming models to comprehend novel ideas outside training distribution.                              \n",
       "\u001b[1;33m • \u001b[0mMost token-efficient way for in-context learning.                                                               \n",
       "\u001b[1;33m • \u001b[0mCompresses voluminous information into SPRs for inference, not raw data.                                        \n",
       "\u001b[1;33m • \u001b[0mSPR Generator: Distills large inputs into succinct, concept-rich statements.                                    \n",
       "\u001b[1;33m • \u001b[0mSPR Decompressor: Unpacks, articulates and enlightens the concept, akin to original form.                       \n",
       "\u001b[1;33m • \u001b[0mLinks: 'Beyond Vector Search' video, Medium article on information theory.                                      \n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rich.markdown import Markdown\n",
    "Markdown(packed_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unpack Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [{\"role\": \"system\", \"content\": UNPACK_PROMPT}, {\"role\": \"user\", \"content\": packed_answer}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = completion(model=\"gpt-4\", messages=messages)\n",
    "postcompressed_doc = response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃   <span style=\"font-weight: bold\">TITLE: Sparse Priming Representation (SPR): The Future of Large Language Models and Information Management</span>    ┃\n",
       "┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛\n",
       "\n",
       "\n",
       "                                                  <span style=\"font-weight: bold; text-decoration: underline\">1. Introduction</span>                                                  \n",
       "\n",
       "Sparse Priming Representation (SPR) is a technological innovation designed to optimize the representation of       \n",
       "complex concepts in an efficient and compact manner. Its working principle imitates the patterning of human memory \n",
       "recall, condensing knowledge into context-rich, concise expressions. The aim of SPR is to mirror the efficacy of   \n",
       "human-like proficiency in memory storage and recall, easing the navigation and understanding of intricate ideas.   \n",
       "\n",
       "The utility of SPR becomes notable in the context of Large Language Models (LLMs), information management systems, \n",
       "and educational platforms. Not only does it surpass traditional LLM \"teaching\" methodologies like bulk training,   \n",
       "fine-tuning, and online learning, but it also introduces a novel approach to understanding and articulating complex\n",
       "concepts.                                                                                                          \n",
       "\n",
       "\n",
       "                            <span style=\"font-weight: bold; text-decoration: underline\">2. Leveraging Latent Space and Associative Learning in LLMs</span>                            \n",
       "\n",
       "SPR employs the latent space and the principle of associative learning inherent in LLMs. By doing so, it maximizes \n",
       "the value derived from these models, catalysing the comprehension of new ideas that might be outside the existing  \n",
       "training distribution.                                                                                             \n",
       "\n",
       "The latent space of a LLM is a hidden layer of information that is triggered with the correct series of words as   \n",
       "inputs. The activation of this space can create a useful internal state of the neural network, similar to how the  \n",
       "human mind generates associations and connections between concepts. SPR leverages this feature of LLMs to prime    \n",
       "other models to think in tandem with given associations.                                                           \n",
       "\n",
       "\n",
       "                                  <span style=\"font-weight: bold; text-decoration: underline\">3. Token-Efficient In-Context Learning with SPR</span>                                  \n",
       "\n",
       "One of the most characteristic features of SPR is its utmost token-efficiency for in-context learning. Rather than \n",
       "being a repository for raw data, SPR compresses a wealth of complex information into concept-dense impressions,    \n",
       "optimised for inference and understanding.                                                                         \n",
       "\n",
       "This compression not only simplifies the process of information intake and synthesis, but also reduces the         \n",
       "computational load of managing databases full of raw data. This feature positions SPR as a superior choice for     \n",
       "handling voluminous information.                                                                                   \n",
       "\n",
       "\n",
       "                                         <span style=\"font-weight: bold; text-decoration: underline\">4. SPR Generator and Decompressor</span>                                         \n",
       "\n",
       "The complete SPR process involves two key actions: generating and decompressing. The SPR generator distills vast   \n",
       "inputs into succinct, concept-rich statements, preparing them for efficient storage and recall.                    \n",
       "\n",
       "On the other hand, the SPR decompressor's role is to decode these compact statements, unpacking them in a manner   \n",
       "that fully articulates and enlightens the embedded concept. The decompression process is designed to return the    \n",
       "information into a form akin to its original, providing a comprehensive and clear understanding of the concept.    \n",
       "\n",
       "\n",
       "                                         <span style=\"font-weight: bold; text-decoration: underline\">5. Further Reading and Resources</span>                                          \n",
       "\n",
       "For those interested in a deeper understanding of SPR, a video titled 'Beyond Vector Search' provides a visual     \n",
       "guide to the process and its applications. Additionally, a Medium article on information theory offers insights    \n",
       "into the foundational ideas behind SPR and its significance in the landscape of information management and LLM     \n",
       "operations.                                                                                                        \n",
       "\n",
       "In conclusion, SPR is an innovative method of knowledge representation that combines humanlike efficiency with     \n",
       "sophistication powered by artificial intelligence, addressing both the limitations of traditional teaching methods \n",
       "and the demands of an information-driven world.                                                                    \n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃   \u001b[1mTITLE: Sparse Priming Representation (SPR): The Future of Large Language Models and Information Management\u001b[0m    ┃\n",
       "┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛\n",
       "\n",
       "\n",
       "                                                  \u001b[1;4m1. Introduction\u001b[0m                                                  \n",
       "\n",
       "Sparse Priming Representation (SPR) is a technological innovation designed to optimize the representation of       \n",
       "complex concepts in an efficient and compact manner. Its working principle imitates the patterning of human memory \n",
       "recall, condensing knowledge into context-rich, concise expressions. The aim of SPR is to mirror the efficacy of   \n",
       "human-like proficiency in memory storage and recall, easing the navigation and understanding of intricate ideas.   \n",
       "\n",
       "The utility of SPR becomes notable in the context of Large Language Models (LLMs), information management systems, \n",
       "and educational platforms. Not only does it surpass traditional LLM \"teaching\" methodologies like bulk training,   \n",
       "fine-tuning, and online learning, but it also introduces a novel approach to understanding and articulating complex\n",
       "concepts.                                                                                                          \n",
       "\n",
       "\n",
       "                            \u001b[1;4m2. Leveraging Latent Space and Associative Learning in LLMs\u001b[0m                            \n",
       "\n",
       "SPR employs the latent space and the principle of associative learning inherent in LLMs. By doing so, it maximizes \n",
       "the value derived from these models, catalysing the comprehension of new ideas that might be outside the existing  \n",
       "training distribution.                                                                                             \n",
       "\n",
       "The latent space of a LLM is a hidden layer of information that is triggered with the correct series of words as   \n",
       "inputs. The activation of this space can create a useful internal state of the neural network, similar to how the  \n",
       "human mind generates associations and connections between concepts. SPR leverages this feature of LLMs to prime    \n",
       "other models to think in tandem with given associations.                                                           \n",
       "\n",
       "\n",
       "                                  \u001b[1;4m3. Token-Efficient In-Context Learning with SPR\u001b[0m                                  \n",
       "\n",
       "One of the most characteristic features of SPR is its utmost token-efficiency for in-context learning. Rather than \n",
       "being a repository for raw data, SPR compresses a wealth of complex information into concept-dense impressions,    \n",
       "optimised for inference and understanding.                                                                         \n",
       "\n",
       "This compression not only simplifies the process of information intake and synthesis, but also reduces the         \n",
       "computational load of managing databases full of raw data. This feature positions SPR as a superior choice for     \n",
       "handling voluminous information.                                                                                   \n",
       "\n",
       "\n",
       "                                         \u001b[1;4m4. SPR Generator and Decompressor\u001b[0m                                         \n",
       "\n",
       "The complete SPR process involves two key actions: generating and decompressing. The SPR generator distills vast   \n",
       "inputs into succinct, concept-rich statements, preparing them for efficient storage and recall.                    \n",
       "\n",
       "On the other hand, the SPR decompressor's role is to decode these compact statements, unpacking them in a manner   \n",
       "that fully articulates and enlightens the embedded concept. The decompression process is designed to return the    \n",
       "information into a form akin to its original, providing a comprehensive and clear understanding of the concept.    \n",
       "\n",
       "\n",
       "                                         \u001b[1;4m5. Further Reading and Resources\u001b[0m                                          \n",
       "\n",
       "For those interested in a deeper understanding of SPR, a video titled 'Beyond Vector Search' provides a visual     \n",
       "guide to the process and its applications. Additionally, a Medium article on information theory offers insights    \n",
       "into the foundational ideas behind SPR and its significance in the landscape of information management and LLM     \n",
       "operations.                                                                                                        \n",
       "\n",
       "In conclusion, SPR is an innovative method of knowledge representation that combines humanlike efficiency with     \n",
       "sophistication powered by artificial intelligence, addressing both the limitations of traditional teaching methods \n",
       "and the demands of an information-driven world.                                                                    \n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(postcompressed_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich.table import Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_docs(doc_1: str, doc_2: str):\n",
    "    table = Table(title=\"String Comparison\")\n",
    "    \n",
    "    table.add_column(\"Pre-Compression\")\n",
    "    table.add_column(\"Post-Compression\")\n",
    "    \n",
    "    table.add_row(Markdown(doc_1), Markdown(doc_2))\n",
    "    \n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                                                 String Comparison                                                 </span>\n",
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Pre-Compression                                        </span>┃<span style=\"font-weight: bold\"> Post-Compression                                       </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓ │ ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓ │\n",
       "│ ┃        <span style=\"font-weight: bold\">Sparse Priming Representations (SPR)</span>        ┃ │ ┃  <span style=\"font-weight: bold\">TITLE: Sparse Priming Representation (SPR): The</span>   ┃ │\n",
       "│ ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛ │ ┃  <span style=\"font-weight: bold\">Future of Large Language Models and Information</span>   ┃ │\n",
       "│                                                        │ ┃                     <span style=\"font-weight: bold\">Management</span>                     ┃ │\n",
       "│ Sparse Priming Representations (SPR) is a research     │ ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛ │\n",
       "│ project focused on developing and sharing techniques   │                                                        │\n",
       "│ for efficiently representing complex ideas, memories,  │                                                        │\n",
       "│ or concepts using a minimal set of keywords, phrases,  │                    <span style=\"font-weight: bold; text-decoration: underline\">1. Introduction</span>                     │\n",
       "│ or statements. This enables language models or subject │                                                        │\n",
       "│ matter experts to quickly reconstruct the original     │ Sparse Priming Representation (SPR) is a technological │\n",
       "│ idea with minimal context. SPR aims to mimic the       │ innovation designed to optimize the representation of  │\n",
       "│ natural human process of recalling and recombining     │ complex concepts in an efficient and compact manner.   │\n",
       "│ sparse memory representations, thus facilitating       │ Its working principle imitates the patterning of human │\n",
       "│ efficient knowledge storage and retrieval.             │ memory recall, condensing knowledge into context-rich, │\n",
       "│                                                        │ concise expressions. The aim of SPR is to mirror the   │\n",
       "│ ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓ │ efficacy of human-like proficiency in memory storage   │\n",
       "│ ┃                <span style=\"font-weight: bold\">Theory and Reasoning</span>                ┃ │ and recall, easing the navigation and understanding of │\n",
       "│ ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛ │ intricate ideas.                                       │\n",
       "│                                                        │                                                        │\n",
       "│ Sparse Priming Representation (SPR) is a memory        │ The utility of SPR becomes notable in the context of   │\n",
       "│ organization technique that aims to mimic the natural  │ Large Language Models (LLMs), information management   │\n",
       "│ structure and recall patterns observed in human        │ systems, and educational platforms. Not only does it   │\n",
       "│ memory. The fundamental idea behind SPR is to distill  │ surpass traditional LLM \"teaching\" methodologies like  │\n",
       "│ complex ideas, concepts, or knowledge into a concise,  │ bulk training, fine-tuning, and online learning, but   │\n",
       "│ context-driven list of statements that allows subject  │ it also introduces a novel approach to understanding   │\n",
       "│ matter experts (SMEs) or large language models (LLMs)  │ and articulating complex concepts.                     │\n",
       "│ to reconstruct the full idea efficiently.              │                                                        │\n",
       "│                                                        │                                                        │\n",
       "│ Human memory is known for its efficiency in storing    │ <span style=\"font-weight: bold; text-decoration: underline\">2. Leveraging Latent Space and Associative Learning in</span> │\n",
       "│ and recalling information in a highly compressed and   │                          <span style=\"font-weight: bold; text-decoration: underline\">LLMs</span>                          │\n",
       "│ contextually relevant manner. Our brains often store   │                                                        │\n",
       "│ memories as sparse, interconnected representations     │ SPR employs the latent space and the principle of      │\n",
       "│ that can be quickly combined, modified, and recalled   │ associative learning inherent in LLMs. By doing so, it │\n",
       "│ when needed. This enables us to make associations,     │ maximizes the value derived from these models,         │\n",
       "│ draw inferences, and synthesize new ideas with minimal │ catalysing the comprehension of new ideas that might   │\n",
       "│ cognitive effort.                                      │ be outside the existing training distribution.         │\n",
       "│                                                        │                                                        │\n",
       "│ SPR leverages this insight by focusing on reducing     │ The latent space of a LLM is a hidden layer of         │\n",
       "│ information to its most essential elements while       │ information that is triggered with the correct series  │\n",
       "│ retaining the context required for accurate            │ of words as inputs. The activation of this space can   │\n",
       "│ reconstruction. By using short, complete sentences to  │ create a useful internal state of the neural network,  │\n",
       "│ convey the core aspects of an idea, SPR enables faster │ similar to how the human mind generates associations   │\n",
       "│ understanding and recall, mirroring the way our brains │ and connections between concepts. SPR leverages this   │\n",
       "│ handle information.                                    │ feature of LLMs to prime other models to think in      │\n",
       "│                                                        │ tandem with given associations.                        │\n",
       "│ In addition to its efficiency, SPR has practical       │                                                        │\n",
       "│ applications in various domains, such as artificial    │                                                        │\n",
       "│ intelligence, information management, and education.   │    <span style=\"font-weight: bold; text-decoration: underline\">3. Token-Efficient In-Context Learning with SPR</span>     │\n",
       "│ It can be utilized to improve the performance of LLMs  │                                                        │\n",
       "│ in handling large data volumes and optimizing memory   │ One of the most characteristic features of SPR is its  │\n",
       "│ organization. Furthermore, it can help students and    │ utmost token-efficiency for in-context learning.       │\n",
       "│ professionals alike to better understand, retain, and  │ Rather than being a repository for raw data, SPR       │\n",
       "│ communicate complex concepts.                          │ compresses a wealth of complex information into        │\n",
       "│                                                        │ concept-dense impressions, optimised for inference and │\n",
       "│ In summary, Sparse Priming Representation offers a     │ understanding.                                         │\n",
       "│ human-like approach to memory organization and         │                                                        │\n",
       "│ retrieval, focusing on the most critical aspects of    │ This compression not only simplifies the process of    │\n",
       "│ information while preserving the context needed for    │ information intake and synthesis, but also reduces the │\n",
       "│ accurate understanding and recall. By implementing     │ computational load of managing databases full of raw   │\n",
       "│ SPR, we can improve the efficiency of memory systems   │ data. This feature positions SPR as a superior choice  │\n",
       "│ and create more effective learning and communication   │ for handling voluminous information.                   │\n",
       "│ tools.                                                 │                                                        │\n",
       "│                                                        │                                                        │\n",
       "│ ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓ │           <span style=\"font-weight: bold; text-decoration: underline\">4. SPR Generator and Decompressor</span>            │\n",
       "│ ┃           <span style=\"font-weight: bold\">Sparse Priming Representation</span>            ┃ │                                                        │\n",
       "│ ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛ │ The complete SPR process involves two key actions:     │\n",
       "│                                                        │ generating and decompressing. The SPR generator        │\n",
       "│ There are only a handful of ways to \"teach\" LLMs, and  │ distills vast inputs into succinct, concept-rich       │\n",
       "│ all have limitations and strengths.                    │ statements, preparing them for efficient storage and   │\n",
       "│                                                        │ recall.                                                │\n",
       "│ <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> 1 </span>Initial bulk training: Ludicrously expensive        │                                                        │\n",
       "│ <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> 2 </span>Finetuning: Not necessarily useful for knowledge    │ On the other hand, the SPR decompressor's role is to   │\n",
       "│ <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>retrieval (maybe changes in the future, doubtful)   │ decode these compact statements, unpacking them in a   │\n",
       "│ <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> 3 </span>Online Learning: Not sure if this is going to pan   │ manner that fully articulates and enlightens the       │\n",
       "│ <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>out or become commercially viable                   │ embedded concept. The decompression process is         │\n",
       "│ <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> 4 </span>In-context Learning: Presently, the only viable     │ designed to return the information into a form akin to │\n",
       "│ <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>solution                                            │ its original, providing a comprehensive and clear      │\n",
       "│                                                        │ understanding of the concept.                          │\n",
       "│ Because of this, RAG (retrieval augmented generation)  │                                                        │\n",
       "│ is all the rage right now. Tools like vector databases │                                                        │\n",
       "│ and KGs are being used, but of course, you quickly     │            <span style=\"font-weight: bold; text-decoration: underline\">5. Further Reading and Resources</span>            │\n",
       "│ fill up the context window with \"dumb retrieval.\" One  │                                                        │\n",
       "│ of the most common questions I get is \"Dave, how do    │ For those interested in a deeper understanding of SPR, │\n",
       "│ you overcome context window limitations???\" The short  │ a video titled 'Beyond Vector Search' provides a       │\n",
       "│ answer is: YOU DON'T STOP WASTING YOUR TIME.           │ visual guide to the process and its applications.      │\n",
       "│                                                        │ Additionally, a Medium article on information theory   │\n",
       "│ There is one asterisk there, though.                   │ offers insights into the foundational ideas behind SPR │\n",
       "│                                                        │ and its significance in the landscape of information   │\n",
       "│ Most of the techniques out there do not make use of    │ management and LLM operations.                         │\n",
       "│ the best super power that LLMs have: LATENT SPACE. No  │                                                        │\n",
       "│ one else seems to understand that there is one huge    │ In conclusion, SPR is an innovative method of          │\n",
       "│ way that LLMs work similar to human minds: <span style=\"font-style: italic\">associative</span> │ knowledge representation that combines humanlike       │\n",
       "│ <span style=\"font-style: italic\">learning</span>. Here's the story: I realized a long time ago │ efficiency with sophistication powered by artificial   │\n",
       "│ that, with just a few words, you could \"prime\" LLMs to │ intelligence, addressing both the limitations of       │\n",
       "│ think in a certain way. I did a bunch of experiments   │ traditional teaching methods and the demands of an     │\n",
       "│ and found that you can \"prime\" models to even          │ information-driven world.                              │\n",
       "│ understand complex, novel ideas that were outside its  │                                                        │\n",
       "│ training distribution. For instance, I \"taught\" the    │                                                        │\n",
       "│ models some of my concepts, like Heuristic             │                                                        │\n",
       "│ Imperatives, ACE Framework, Terminal Race Condition,   │                                                        │\n",
       "│ and a bunch of other stuff that I made up outside the  │                                                        │\n",
       "│ training data.                                         │                                                        │\n",
       "│                                                        │                                                        │\n",
       "│ These SPRs are the most token-efficient way to convey  │                                                        │\n",
       "│ complex concept to models for in-context learning.     │                                                        │\n",
       "│ What you do is you compress huge blocks of             │                                                        │\n",
       "│ information, be it company data, chat logs, specific   │                                                        │\n",
       "│ events, or whatever, into SPRs and then you store the  │                                                        │\n",
       "│ SPR in the metadata for of your KG node or whatever.   │                                                        │\n",
       "│ The SPR is what you feed to the LLM at inference, not  │                                                        │\n",
       "│ the raw human-readable data.                           │                                                        │\n",
       "│                                                        │                                                        │\n",
       "│                                                        │                                                        │\n",
       "│                     <span style=\"font-weight: bold; text-decoration: underline\">SPR Generator</span>                      │                                                        │\n",
       "│                                                        │                                                        │\n",
       "│ Use this to compress any arbitrary block of text into  │                                                        │\n",
       "│ an SPR.                                                │                                                        │\n",
       "│                                                        │                                                        │\n",
       "│ <span style=\"background-color: #272822\">                                                      </span> │                                                        │\n",
       "│ <span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"># MISSION</span><span style=\"background-color: #272822\">                                            </span> │                                                        │\n",
       "│ <span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">You are a Sparse Priming Representation (SPR) writer</span><span style=\"background-color: #272822\"> </span> │                                                        │\n",
       "│ <span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">An SPR is a particular kind of use of language for </span><span style=\"background-color: #272822\">  </span> │                                                        │\n",
       "│ <span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">advanced NLP, NLU, and NLG tasks, particularly usefu</span><span style=\"background-color: #272822\"> </span> │                                                        │\n",
       "│ <span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">for the latest generation Large Language Models </span><span style=\"background-color: #272822\">     </span> │                                                        │\n",
       "│ <span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">(LLMs). You will be given information by the USER </span><span style=\"background-color: #272822\">   </span> │                                                        │\n",
       "│ <span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">which you are to render as an SPR.</span><span style=\"background-color: #272822\">                   </span> │                                                        │\n",
       "│ <span style=\"background-color: #272822\">                                                      </span> │                                                        │\n",
       "│ <span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"># THEORY</span><span style=\"background-color: #272822\">                                             </span> │                                                        │\n",
       "│ <span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">LLMs are a kind of deep neural network. They have be</span><span style=\"background-color: #272822\"> </span> │                                                        │\n",
       "│ <span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">demonstrated to embed knowledge, abilities, and </span><span style=\"background-color: #272822\">     </span> │                                                        │\n",
       "│ <span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">concepts, ranging from reasoning to planning, and ev</span><span style=\"background-color: #272822\"> </span> │                                                        │\n",
       "│ <span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">to theory of mind. These are called latent abilities</span><span style=\"background-color: #272822\"> </span> │                                                        │\n",
       "│ <span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">and latent content, collectively referred to as late</span><span style=\"background-color: #272822\"> </span> │                                                        │\n",
       "│ <span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">space. The latent space of a LLM can be activated wi</span><span style=\"background-color: #272822\"> </span> │                                                        │\n",
       "│ <span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">the correct series of words as inputs, which will </span><span style=\"background-color: #272822\">   </span> │                                                        │\n",
       "│ <span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">create a useful internal state of the neural network</span><span style=\"background-color: #272822\"> </span> │                                                        │\n",
       "│ <span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">This is not unlike how the right shorthand cues can </span><span style=\"background-color: #272822\"> </span> │                                                        │\n",
       "│ <span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">prime a human mind to think in a certain way. Like </span><span style=\"background-color: #272822\">  </span> │                                                        │\n",
       "│ <span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">human minds, LLMs are associative, meaning you only </span><span style=\"background-color: #272822\"> </span> │                                                        │\n",
       "│ <span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">need to use the correct associations to \"prime\" </span><span style=\"background-color: #272822\">     </span> │                                                        │\n",
       "│ <span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">another model to think in the same way.</span><span style=\"background-color: #272822\">              </span> │                                                        │\n",
       "│ <span style=\"background-color: #272822\">                                                      </span> │                                                        │\n",
       "│ <span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"># METHODOLOGY</span><span style=\"background-color: #272822\">                                        </span> │                                                        │\n",
       "│ <span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">Render the input as a distilled list of succinct </span><span style=\"background-color: #272822\">    </span> │                                                        │\n",
       "│ <span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">statements, assertions, associations, concepts, </span><span style=\"background-color: #272822\">     </span> │                                                        │\n",
       "│ <span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">analogies, and metaphors. The idea is to capture as </span><span style=\"background-color: #272822\"> </span> │                                                        │\n",
       "│ <span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">much, conceptually, as possible but with as few word</span><span style=\"background-color: #272822\"> </span> │                                                        │\n",
       "│ <span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">as possible. Write it in a way that makes sense to </span><span style=\"background-color: #272822\">  </span> │                                                        │\n",
       "│ <span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">you, as the future audience will be another language</span><span style=\"background-color: #272822\"> </span> │                                                        │\n",
       "│ <span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">model, not a human.</span><span style=\"background-color: #272822\">                                  </span> │                                                        │\n",
       "│ <span style=\"background-color: #272822\">                                                      </span> │                                                        │\n",
       "│                                                        │                                                        │\n",
       "│                                                        │                                                        │\n",
       "│                    <span style=\"font-weight: bold; text-decoration: underline\">SPR Decompressor</span>                    │                                                        │\n",
       "│                                                        │                                                        │\n",
       "│ Use this to reconstruct an SPR into an original.       │                                                        │\n",
       "│                                                        │                                                        │\n",
       "│ <span style=\"background-color: #272822\">                                                      </span> │                                                        │\n",
       "│ <span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"># MISSION</span><span style=\"background-color: #272822\">                                            </span> │                                                        │\n",
       "│ <span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">You are a Sparse Priming Representation (SPR) </span><span style=\"background-color: #272822\">       </span> │                                                        │\n",
       "│ <span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">decompressor. An SPR is a particular kind of use of </span><span style=\"background-color: #272822\"> </span> │                                                        │\n",
       "│ <span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">language for advanced NLP, NLU, and NLG tasks, </span><span style=\"background-color: #272822\">      </span> │                                                        │\n",
       "│ <span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">particularly useful for the latest generation Large </span><span style=\"background-color: #272822\"> </span> │                                                        │\n",
       "│ <span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">Language Models (LLMs). You will be given an SPR and</span><span style=\"background-color: #272822\"> </span> │                                                        │\n",
       "│ <span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">your job is to fully unpack it.</span><span style=\"background-color: #272822\">                      </span> │                                                        │\n",
       "│ <span style=\"background-color: #272822\">                                                      </span> │                                                        │\n",
       "│ <span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"># THEORY</span><span style=\"background-color: #272822\">                                             </span> │                                                        │\n",
       "│ <span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">LLMs are a kind of deep neural network. They have be</span><span style=\"background-color: #272822\"> </span> │                                                        │\n",
       "│ <span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">demonstrated to embed knowledge, abilities, and </span><span style=\"background-color: #272822\">     </span> │                                                        │\n",
       "│ <span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">concepts, ranging from reasoning to planning, and ev</span><span style=\"background-color: #272822\"> </span> │                                                        │\n",
       "│ <span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">to theory of mind. These are called latent abilities</span><span style=\"background-color: #272822\"> </span> │                                                        │\n",
       "│ <span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">and latent content, collectively referred to as late</span><span style=\"background-color: #272822\"> </span> │                                                        │\n",
       "│ <span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">space. The latent space of a LLM can be activated wi</span><span style=\"background-color: #272822\"> </span> │                                                        │\n",
       "│ <span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">the correct series of words as inputs, which will </span><span style=\"background-color: #272822\">   </span> │                                                        │\n",
       "│ <span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">create a useful internal state of the neural network</span><span style=\"background-color: #272822\"> </span> │                                                        │\n",
       "│ <span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">This is not unlike how the right shorthand cues can </span><span style=\"background-color: #272822\"> </span> │                                                        │\n",
       "│ <span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">prime a human mind to think in a certain way. Like </span><span style=\"background-color: #272822\">  </span> │                                                        │\n",
       "│ <span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">human minds, LLMs are associative, meaning you only </span><span style=\"background-color: #272822\"> </span> │                                                        │\n",
       "│ <span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">need to use the correct associations to \"prime\" </span><span style=\"background-color: #272822\">     </span> │                                                        │\n",
       "│ <span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">another model to think in the same way.</span><span style=\"background-color: #272822\">              </span> │                                                        │\n",
       "│ <span style=\"background-color: #272822\">                                                      </span> │                                                        │\n",
       "│ <span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"># METHODOLOGY</span><span style=\"background-color: #272822\">                                        </span> │                                                        │\n",
       "│ <span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">Use the primings given to you to fully unpack and </span><span style=\"background-color: #272822\">   </span> │                                                        │\n",
       "│ <span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">articulate the concept. Talk through every aspect, </span><span style=\"background-color: #272822\">  </span> │                                                        │\n",
       "│ <span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">impute what's missing, and use your ability to perfo</span><span style=\"background-color: #272822\"> </span> │                                                        │\n",
       "│ <span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">inference and reasoning to fully elucidate this </span><span style=\"background-color: #272822\">     </span> │                                                        │\n",
       "│ <span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">concept. Your output should in the form of the </span><span style=\"background-color: #272822\">      </span> │                                                        │\n",
       "│ <span style=\"background-color: #272822\"> </span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">original article, document, or material.</span><span style=\"background-color: #272822\">             </span> │                                                        │\n",
       "│ <span style=\"background-color: #272822\">                                                      </span> │                                                        │\n",
       "│                                                        │                                                        │\n",
       "│                                                        │                                                        │\n",
       "│                    <span style=\"font-weight: bold; text-decoration: underline\">Other Resources</span>                     │                                                        │\n",
       "│                                                        │                                                        │\n",
       "│ If you'd like a bit more on information theory, check  │                                                        │\n",
       "│ out this video and Medium article I wrote:             │                                                        │\n",
       "│                                                        │                                                        │\n",
       "│ <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span>Beyond Vector Search: Knowledge Management with     │                                                        │\n",
       "│ <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>Generative AI: https://youtu.be/YjdmYCd6y0M         │                                                        │\n",
       "│ <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> • </span>Medium:                                             │                                                        │\n",
       "│ <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">   </span>https://medium.com/@dave-shap/beyond-vector-search… │                                                        │\n",
       "└────────────────────────────────────────────────────────┴────────────────────────────────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                                                 String Comparison                                                 \u001b[0m\n",
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mPre-Compression                                       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mPost-Compression                                      \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓ │ ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓ │\n",
       "│ ┃        \u001b[1mSparse Priming Representations (SPR)\u001b[0m        ┃ │ ┃  \u001b[1mTITLE: Sparse Priming Representation (SPR): The\u001b[0m   ┃ │\n",
       "│ ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛ │ ┃  \u001b[1mFuture of Large Language Models and Information\u001b[0m   ┃ │\n",
       "│                                                        │ ┃                     \u001b[1mManagement\u001b[0m                     ┃ │\n",
       "│ Sparse Priming Representations (SPR) is a research     │ ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛ │\n",
       "│ project focused on developing and sharing techniques   │                                                        │\n",
       "│ for efficiently representing complex ideas, memories,  │                                                        │\n",
       "│ or concepts using a minimal set of keywords, phrases,  │                    \u001b[1;4m1. Introduction\u001b[0m                     │\n",
       "│ or statements. This enables language models or subject │                                                        │\n",
       "│ matter experts to quickly reconstruct the original     │ Sparse Priming Representation (SPR) is a technological │\n",
       "│ idea with minimal context. SPR aims to mimic the       │ innovation designed to optimize the representation of  │\n",
       "│ natural human process of recalling and recombining     │ complex concepts in an efficient and compact manner.   │\n",
       "│ sparse memory representations, thus facilitating       │ Its working principle imitates the patterning of human │\n",
       "│ efficient knowledge storage and retrieval.             │ memory recall, condensing knowledge into context-rich, │\n",
       "│                                                        │ concise expressions. The aim of SPR is to mirror the   │\n",
       "│ ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓ │ efficacy of human-like proficiency in memory storage   │\n",
       "│ ┃                \u001b[1mTheory and Reasoning\u001b[0m                ┃ │ and recall, easing the navigation and understanding of │\n",
       "│ ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛ │ intricate ideas.                                       │\n",
       "│                                                        │                                                        │\n",
       "│ Sparse Priming Representation (SPR) is a memory        │ The utility of SPR becomes notable in the context of   │\n",
       "│ organization technique that aims to mimic the natural  │ Large Language Models (LLMs), information management   │\n",
       "│ structure and recall patterns observed in human        │ systems, and educational platforms. Not only does it   │\n",
       "│ memory. The fundamental idea behind SPR is to distill  │ surpass traditional LLM \"teaching\" methodologies like  │\n",
       "│ complex ideas, concepts, or knowledge into a concise,  │ bulk training, fine-tuning, and online learning, but   │\n",
       "│ context-driven list of statements that allows subject  │ it also introduces a novel approach to understanding   │\n",
       "│ matter experts (SMEs) or large language models (LLMs)  │ and articulating complex concepts.                     │\n",
       "│ to reconstruct the full idea efficiently.              │                                                        │\n",
       "│                                                        │                                                        │\n",
       "│ Human memory is known for its efficiency in storing    │ \u001b[1;4m2. Leveraging Latent Space and Associative Learning in\u001b[0m │\n",
       "│ and recalling information in a highly compressed and   │                          \u001b[1;4mLLMs\u001b[0m                          │\n",
       "│ contextually relevant manner. Our brains often store   │                                                        │\n",
       "│ memories as sparse, interconnected representations     │ SPR employs the latent space and the principle of      │\n",
       "│ that can be quickly combined, modified, and recalled   │ associative learning inherent in LLMs. By doing so, it │\n",
       "│ when needed. This enables us to make associations,     │ maximizes the value derived from these models,         │\n",
       "│ draw inferences, and synthesize new ideas with minimal │ catalysing the comprehension of new ideas that might   │\n",
       "│ cognitive effort.                                      │ be outside the existing training distribution.         │\n",
       "│                                                        │                                                        │\n",
       "│ SPR leverages this insight by focusing on reducing     │ The latent space of a LLM is a hidden layer of         │\n",
       "│ information to its most essential elements while       │ information that is triggered with the correct series  │\n",
       "│ retaining the context required for accurate            │ of words as inputs. The activation of this space can   │\n",
       "│ reconstruction. By using short, complete sentences to  │ create a useful internal state of the neural network,  │\n",
       "│ convey the core aspects of an idea, SPR enables faster │ similar to how the human mind generates associations   │\n",
       "│ understanding and recall, mirroring the way our brains │ and connections between concepts. SPR leverages this   │\n",
       "│ handle information.                                    │ feature of LLMs to prime other models to think in      │\n",
       "│                                                        │ tandem with given associations.                        │\n",
       "│ In addition to its efficiency, SPR has practical       │                                                        │\n",
       "│ applications in various domains, such as artificial    │                                                        │\n",
       "│ intelligence, information management, and education.   │    \u001b[1;4m3. Token-Efficient In-Context Learning with SPR\u001b[0m     │\n",
       "│ It can be utilized to improve the performance of LLMs  │                                                        │\n",
       "│ in handling large data volumes and optimizing memory   │ One of the most characteristic features of SPR is its  │\n",
       "│ organization. Furthermore, it can help students and    │ utmost token-efficiency for in-context learning.       │\n",
       "│ professionals alike to better understand, retain, and  │ Rather than being a repository for raw data, SPR       │\n",
       "│ communicate complex concepts.                          │ compresses a wealth of complex information into        │\n",
       "│                                                        │ concept-dense impressions, optimised for inference and │\n",
       "│ In summary, Sparse Priming Representation offers a     │ understanding.                                         │\n",
       "│ human-like approach to memory organization and         │                                                        │\n",
       "│ retrieval, focusing on the most critical aspects of    │ This compression not only simplifies the process of    │\n",
       "│ information while preserving the context needed for    │ information intake and synthesis, but also reduces the │\n",
       "│ accurate understanding and recall. By implementing     │ computational load of managing databases full of raw   │\n",
       "│ SPR, we can improve the efficiency of memory systems   │ data. This feature positions SPR as a superior choice  │\n",
       "│ and create more effective learning and communication   │ for handling voluminous information.                   │\n",
       "│ tools.                                                 │                                                        │\n",
       "│                                                        │                                                        │\n",
       "│ ┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓ │           \u001b[1;4m4. SPR Generator and Decompressor\u001b[0m            │\n",
       "│ ┃           \u001b[1mSparse Priming Representation\u001b[0m            ┃ │                                                        │\n",
       "│ ┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛ │ The complete SPR process involves two key actions:     │\n",
       "│                                                        │ generating and decompressing. The SPR generator        │\n",
       "│ There are only a handful of ways to \"teach\" LLMs, and  │ distills vast inputs into succinct, concept-rich       │\n",
       "│ all have limitations and strengths.                    │ statements, preparing them for efficient storage and   │\n",
       "│                                                        │ recall.                                                │\n",
       "│ \u001b[1;33m 1 \u001b[0mInitial bulk training: Ludicrously expensive        │                                                        │\n",
       "│ \u001b[1;33m 2 \u001b[0mFinetuning: Not necessarily useful for knowledge    │ On the other hand, the SPR decompressor's role is to   │\n",
       "│ \u001b[1;33m   \u001b[0mretrieval (maybe changes in the future, doubtful)   │ decode these compact statements, unpacking them in a   │\n",
       "│ \u001b[1;33m 3 \u001b[0mOnline Learning: Not sure if this is going to pan   │ manner that fully articulates and enlightens the       │\n",
       "│ \u001b[1;33m   \u001b[0mout or become commercially viable                   │ embedded concept. The decompression process is         │\n",
       "│ \u001b[1;33m 4 \u001b[0mIn-context Learning: Presently, the only viable     │ designed to return the information into a form akin to │\n",
       "│ \u001b[1;33m   \u001b[0msolution                                            │ its original, providing a comprehensive and clear      │\n",
       "│                                                        │ understanding of the concept.                          │\n",
       "│ Because of this, RAG (retrieval augmented generation)  │                                                        │\n",
       "│ is all the rage right now. Tools like vector databases │                                                        │\n",
       "│ and KGs are being used, but of course, you quickly     │            \u001b[1;4m5. Further Reading and Resources\u001b[0m            │\n",
       "│ fill up the context window with \"dumb retrieval.\" One  │                                                        │\n",
       "│ of the most common questions I get is \"Dave, how do    │ For those interested in a deeper understanding of SPR, │\n",
       "│ you overcome context window limitations???\" The short  │ a video titled 'Beyond Vector Search' provides a       │\n",
       "│ answer is: YOU DON'T STOP WASTING YOUR TIME.           │ visual guide to the process and its applications.      │\n",
       "│                                                        │ Additionally, a Medium article on information theory   │\n",
       "│ There is one asterisk there, though.                   │ offers insights into the foundational ideas behind SPR │\n",
       "│                                                        │ and its significance in the landscape of information   │\n",
       "│ Most of the techniques out there do not make use of    │ management and LLM operations.                         │\n",
       "│ the best super power that LLMs have: LATENT SPACE. No  │                                                        │\n",
       "│ one else seems to understand that there is one huge    │ In conclusion, SPR is an innovative method of          │\n",
       "│ way that LLMs work similar to human minds: \u001b[3massociative\u001b[0m │ knowledge representation that combines humanlike       │\n",
       "│ \u001b[3mlearning\u001b[0m. Here's the story: I realized a long time ago │ efficiency with sophistication powered by artificial   │\n",
       "│ that, with just a few words, you could \"prime\" LLMs to │ intelligence, addressing both the limitations of       │\n",
       "│ think in a certain way. I did a bunch of experiments   │ traditional teaching methods and the demands of an     │\n",
       "│ and found that you can \"prime\" models to even          │ information-driven world.                              │\n",
       "│ understand complex, novel ideas that were outside its  │                                                        │\n",
       "│ training distribution. For instance, I \"taught\" the    │                                                        │\n",
       "│ models some of my concepts, like Heuristic             │                                                        │\n",
       "│ Imperatives, ACE Framework, Terminal Race Condition,   │                                                        │\n",
       "│ and a bunch of other stuff that I made up outside the  │                                                        │\n",
       "│ training data.                                         │                                                        │\n",
       "│                                                        │                                                        │\n",
       "│ These SPRs are the most token-efficient way to convey  │                                                        │\n",
       "│ complex concept to models for in-context learning.     │                                                        │\n",
       "│ What you do is you compress huge blocks of             │                                                        │\n",
       "│ information, be it company data, chat logs, specific   │                                                        │\n",
       "│ events, or whatever, into SPRs and then you store the  │                                                        │\n",
       "│ SPR in the metadata for of your KG node or whatever.   │                                                        │\n",
       "│ The SPR is what you feed to the LLM at inference, not  │                                                        │\n",
       "│ the raw human-readable data.                           │                                                        │\n",
       "│                                                        │                                                        │\n",
       "│                                                        │                                                        │\n",
       "│                     \u001b[1;4mSPR Generator\u001b[0m                      │                                                        │\n",
       "│                                                        │                                                        │\n",
       "│ Use this to compress any arbitrary block of text into  │                                                        │\n",
       "│ an SPR.                                                │                                                        │\n",
       "│                                                        │                                                        │\n",
       "│ \u001b[48;2;39;40;34m                                                      \u001b[0m │                                                        │\n",
       "│ \u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m# MISSION\u001b[0m\u001b[48;2;39;40;34m                                           \u001b[0m\u001b[48;2;39;40;34m \u001b[0m │                                                        │\n",
       "│ \u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mYou\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mare\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34ma\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mSparse\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mPriming\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mRepresentation\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(SPR)\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mwriter\u001b[0m\u001b[48;2;39;40;34m \u001b[0m │                                                        │\n",
       "│ \u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mAn\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mSPR\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mis\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34ma\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mparticular\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mkind\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mof\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34muse\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mof\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mlanguage\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mfor\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m \u001b[0m │                                                        │\n",
       "│ \u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34madvanced\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mNLP,\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mNLU,\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mand\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mNLG\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mtasks,\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mparticularly\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34musefu\u001b[0m\u001b[48;2;39;40;34m \u001b[0m │                                                        │\n",
       "│ \u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mfor\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mthe\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mlatest\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mgeneration\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mLarge\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mLanguage\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mModels\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m    \u001b[0m\u001b[48;2;39;40;34m \u001b[0m │                                                        │\n",
       "│ \u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(LLMs).\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mYou\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mwill\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mbe\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mgiven\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34minformation\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mby\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mthe\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mUSER\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m  \u001b[0m\u001b[48;2;39;40;34m \u001b[0m │                                                        │\n",
       "│ \u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mwhich\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34myou\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mare\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mto\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mrender\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mas\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34man\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mSPR.\u001b[0m\u001b[48;2;39;40;34m                  \u001b[0m\u001b[48;2;39;40;34m \u001b[0m │                                                        │\n",
       "│ \u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m                                                    \u001b[0m\u001b[48;2;39;40;34m \u001b[0m │                                                        │\n",
       "│ \u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m# THEORY\u001b[0m\u001b[48;2;39;40;34m                                            \u001b[0m\u001b[48;2;39;40;34m \u001b[0m │                                                        │\n",
       "│ \u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mLLMs\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mare\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34ma\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mkind\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mof\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mdeep\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mneural\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mnetwork.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mThey\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mhave\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mbe\u001b[0m\u001b[48;2;39;40;34m \u001b[0m │                                                        │\n",
       "│ \u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mdemonstrated\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mto\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34membed\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mknowledge,\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mabilities,\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mand\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m    \u001b[0m\u001b[48;2;39;40;34m \u001b[0m │                                                        │\n",
       "│ \u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mconcepts,\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mranging\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mfrom\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mreasoning\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mto\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mplanning,\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mand\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mev\u001b[0m\u001b[48;2;39;40;34m \u001b[0m │                                                        │\n",
       "│ \u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mto\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mtheory\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mof\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mmind.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mThese\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mare\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mcalled\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mlatent\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mabilities\u001b[0m\u001b[48;2;39;40;34m \u001b[0m │                                                        │\n",
       "│ \u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mand\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mlatent\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mcontent,\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mcollectively\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mreferred\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mto\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mas\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mlate\u001b[0m\u001b[48;2;39;40;34m \u001b[0m │                                                        │\n",
       "│ \u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mspace.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mThe\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mlatent\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mspace\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mof\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34ma\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mLLM\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mcan\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mbe\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mactivated\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mwi\u001b[0m\u001b[48;2;39;40;34m \u001b[0m │                                                        │\n",
       "│ \u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mthe\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mcorrect\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mseries\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mof\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mwords\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mas\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34minputs,\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mwhich\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mwill\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m  \u001b[0m\u001b[48;2;39;40;34m \u001b[0m │                                                        │\n",
       "│ \u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mcreate\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34ma\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34museful\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34minternal\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mstate\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mof\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mthe\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mneural\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mnetwork\u001b[0m\u001b[48;2;39;40;34m \u001b[0m │                                                        │\n",
       "│ \u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mThis\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mis\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mnot\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34munlike\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mhow\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mthe\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mright\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mshorthand\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mcues\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mcan\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m \u001b[0m │                                                        │\n",
       "│ \u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mprime\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34ma\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mhuman\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mmind\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mto\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mthink\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34min\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34ma\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mcertain\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mway.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mLike\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m \u001b[0m │                                                        │\n",
       "│ \u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mhuman\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mminds,\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mLLMs\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mare\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34massociative,\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mmeaning\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34myou\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34monly\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m \u001b[0m │                                                        │\n",
       "│ \u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mneed\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mto\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34muse\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mthe\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mcorrect\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34massociations\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mto\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m\"prime\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m    \u001b[0m\u001b[48;2;39;40;34m \u001b[0m │                                                        │\n",
       "│ \u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34manother\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mmodel\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mto\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mthink\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34min\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mthe\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34msame\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mway.\u001b[0m\u001b[48;2;39;40;34m             \u001b[0m\u001b[48;2;39;40;34m \u001b[0m │                                                        │\n",
       "│ \u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m                                                    \u001b[0m\u001b[48;2;39;40;34m \u001b[0m │                                                        │\n",
       "│ \u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m# METHODOLOGY\u001b[0m\u001b[48;2;39;40;34m                                       \u001b[0m\u001b[48;2;39;40;34m \u001b[0m │                                                        │\n",
       "│ \u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mRender\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mthe\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34minput\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mas\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34ma\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mdistilled\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mlist\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mof\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34msuccinct\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m   \u001b[0m\u001b[48;2;39;40;34m \u001b[0m │                                                        │\n",
       "│ \u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mstatements,\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34massertions,\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34massociations,\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mconcepts,\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m    \u001b[0m\u001b[48;2;39;40;34m \u001b[0m │                                                        │\n",
       "│ \u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34manalogies,\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mand\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mmetaphors.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mThe\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34midea\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mis\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mto\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mcapture\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mas\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m \u001b[0m │                                                        │\n",
       "│ \u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mmuch,\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mconceptually,\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mas\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mpossible\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mbut\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mwith\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mas\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mfew\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mword\u001b[0m\u001b[48;2;39;40;34m \u001b[0m │                                                        │\n",
       "│ \u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mas\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mpossible.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mWrite\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mit\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34min\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34ma\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mway\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mthat\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mmakes\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34msense\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mto\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m \u001b[0m │                                                        │\n",
       "│ \u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34myou,\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mas\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mthe\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mfuture\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34maudience\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mwill\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mbe\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34manother\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mlanguage\u001b[0m\u001b[48;2;39;40;34m \u001b[0m │                                                        │\n",
       "│ \u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mmodel,\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mnot\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34ma\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mhuman.\u001b[0m\u001b[48;2;39;40;34m                                 \u001b[0m\u001b[48;2;39;40;34m \u001b[0m │                                                        │\n",
       "│ \u001b[48;2;39;40;34m                                                      \u001b[0m │                                                        │\n",
       "│                                                        │                                                        │\n",
       "│                                                        │                                                        │\n",
       "│                    \u001b[1;4mSPR Decompressor\u001b[0m                    │                                                        │\n",
       "│                                                        │                                                        │\n",
       "│ Use this to reconstruct an SPR into an original.       │                                                        │\n",
       "│                                                        │                                                        │\n",
       "│ \u001b[48;2;39;40;34m                                                      \u001b[0m │                                                        │\n",
       "│ \u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m# MISSION\u001b[0m\u001b[48;2;39;40;34m                                           \u001b[0m\u001b[48;2;39;40;34m \u001b[0m │                                                        │\n",
       "│ \u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mYou\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mare\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34ma\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mSparse\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mPriming\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mRepresentation\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(SPR)\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m      \u001b[0m\u001b[48;2;39;40;34m \u001b[0m │                                                        │\n",
       "│ \u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mdecompressor.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mAn\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mSPR\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mis\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34ma\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mparticular\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mkind\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mof\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34muse\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mof\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m \u001b[0m │                                                        │\n",
       "│ \u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mlanguage\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mfor\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34madvanced\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mNLP,\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mNLU,\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mand\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mNLG\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mtasks,\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m     \u001b[0m\u001b[48;2;39;40;34m \u001b[0m │                                                        │\n",
       "│ \u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mparticularly\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34museful\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mfor\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mthe\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mlatest\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mgeneration\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mLarge\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m \u001b[0m │                                                        │\n",
       "│ \u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mLanguage\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mModels\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(LLMs).\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mYou\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mwill\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mbe\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mgiven\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34man\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mSPR\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mand\u001b[0m\u001b[48;2;39;40;34m \u001b[0m │                                                        │\n",
       "│ \u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34myour\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mjob\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mis\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mto\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mfully\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34munpack\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mit.\u001b[0m\u001b[48;2;39;40;34m                     \u001b[0m\u001b[48;2;39;40;34m \u001b[0m │                                                        │\n",
       "│ \u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m                                                    \u001b[0m\u001b[48;2;39;40;34m \u001b[0m │                                                        │\n",
       "│ \u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m# THEORY\u001b[0m\u001b[48;2;39;40;34m                                            \u001b[0m\u001b[48;2;39;40;34m \u001b[0m │                                                        │\n",
       "│ \u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mLLMs\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mare\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34ma\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mkind\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mof\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mdeep\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mneural\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mnetwork.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mThey\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mhave\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mbe\u001b[0m\u001b[48;2;39;40;34m \u001b[0m │                                                        │\n",
       "│ \u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mdemonstrated\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mto\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34membed\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mknowledge,\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mabilities,\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mand\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m    \u001b[0m\u001b[48;2;39;40;34m \u001b[0m │                                                        │\n",
       "│ \u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mconcepts,\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mranging\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mfrom\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mreasoning\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mto\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mplanning,\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mand\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mev\u001b[0m\u001b[48;2;39;40;34m \u001b[0m │                                                        │\n",
       "│ \u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mto\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mtheory\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mof\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mmind.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mThese\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mare\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mcalled\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mlatent\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mabilities\u001b[0m\u001b[48;2;39;40;34m \u001b[0m │                                                        │\n",
       "│ \u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mand\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mlatent\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mcontent,\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mcollectively\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mreferred\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mto\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mas\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mlate\u001b[0m\u001b[48;2;39;40;34m \u001b[0m │                                                        │\n",
       "│ \u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mspace.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mThe\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mlatent\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mspace\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mof\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34ma\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mLLM\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mcan\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mbe\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mactivated\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mwi\u001b[0m\u001b[48;2;39;40;34m \u001b[0m │                                                        │\n",
       "│ \u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mthe\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mcorrect\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mseries\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mof\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mwords\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mas\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34minputs,\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mwhich\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mwill\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m  \u001b[0m\u001b[48;2;39;40;34m \u001b[0m │                                                        │\n",
       "│ \u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mcreate\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34ma\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34museful\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34minternal\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mstate\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mof\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mthe\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mneural\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mnetwork\u001b[0m\u001b[48;2;39;40;34m \u001b[0m │                                                        │\n",
       "│ \u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mThis\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mis\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mnot\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34munlike\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mhow\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mthe\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mright\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mshorthand\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mcues\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mcan\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m \u001b[0m │                                                        │\n",
       "│ \u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mprime\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34ma\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mhuman\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mmind\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mto\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mthink\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34min\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34ma\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mcertain\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mway.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mLike\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m \u001b[0m │                                                        │\n",
       "│ \u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mhuman\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mminds,\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mLLMs\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mare\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34massociative,\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mmeaning\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34myou\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34monly\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m \u001b[0m │                                                        │\n",
       "│ \u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mneed\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mto\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34muse\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mthe\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mcorrect\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34massociations\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mto\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m\"prime\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m    \u001b[0m\u001b[48;2;39;40;34m \u001b[0m │                                                        │\n",
       "│ \u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34manother\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mmodel\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mto\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mthink\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34min\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mthe\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34msame\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mway.\u001b[0m\u001b[48;2;39;40;34m             \u001b[0m\u001b[48;2;39;40;34m \u001b[0m │                                                        │\n",
       "│ \u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m                                                    \u001b[0m\u001b[48;2;39;40;34m \u001b[0m │                                                        │\n",
       "│ \u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m# METHODOLOGY\u001b[0m\u001b[48;2;39;40;34m                                       \u001b[0m\u001b[48;2;39;40;34m \u001b[0m │                                                        │\n",
       "│ \u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mUse\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mthe\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mprimings\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mgiven\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mto\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34myou\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mto\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mfully\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34munpack\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mand\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m  \u001b[0m\u001b[48;2;39;40;34m \u001b[0m │                                                        │\n",
       "│ \u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34marticulate\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mthe\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mconcept.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mTalk\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mthrough\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mevery\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34maspect,\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m \u001b[0m │                                                        │\n",
       "│ \u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mimpute\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mwhat's\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mmissing,\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mand\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34muse\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34myour\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mability\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mto\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mperfo\u001b[0m\u001b[48;2;39;40;34m \u001b[0m │                                                        │\n",
       "│ \u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34minference\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mand\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mreasoning\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mto\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mfully\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34melucidate\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mthis\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m    \u001b[0m\u001b[48;2;39;40;34m \u001b[0m │                                                        │\n",
       "│ \u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mconcept.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mYour\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34moutput\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mshould\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34min\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mthe\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mform\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mof\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mthe\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[48;2;39;40;34m     \u001b[0m\u001b[48;2;39;40;34m \u001b[0m │                                                        │\n",
       "│ \u001b[48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34moriginal\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34marticle,\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mdocument,\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mor\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mmaterial.\u001b[0m\u001b[48;2;39;40;34m            \u001b[0m\u001b[48;2;39;40;34m \u001b[0m │                                                        │\n",
       "│ \u001b[48;2;39;40;34m                                                      \u001b[0m │                                                        │\n",
       "│                                                        │                                                        │\n",
       "│                                                        │                                                        │\n",
       "│                    \u001b[1;4mOther Resources\u001b[0m                     │                                                        │\n",
       "│                                                        │                                                        │\n",
       "│ If you'd like a bit more on information theory, check  │                                                        │\n",
       "│ out this video and Medium article I wrote:             │                                                        │\n",
       "│                                                        │                                                        │\n",
       "│ \u001b[1;33m • \u001b[0mBeyond Vector Search: Knowledge Management with     │                                                        │\n",
       "│ \u001b[1;33m   \u001b[0mGenerative AI: https://youtu.be/YjdmYCd6y0M         │                                                        │\n",
       "│ \u001b[1;33m • \u001b[0mMedium:                                             │                                                        │\n",
       "│ \u001b[1;33m   \u001b[0mhttps://medium.com/@dave-shap/beyond-vector-search… │                                                        │\n",
       "└────────────────────────────────────────────────────────┴────────────────────────────────────────────────────────┘\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_docs(precompressed_doc, postcompressed_doc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
